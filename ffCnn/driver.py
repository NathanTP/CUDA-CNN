#XXX This is pseudocode for now to help me think
import kaas
import cnn

#==============================================================================
# This is setup that would really be out-of-band. In theory this stuff would
# already be in the data layer.
#==============================================================================
imgs, lbls = cnn.loadMnist()

kaas.kv.put("imgs", imgs)
kaas.kv.put("lbls", lbls)


# KV will have a bunch of "l#_params" entries for the serialized layer
# descriptions (dimX, dimY, dimOut, weights, biases)
layers = []
for i in range(3):
    layers[i] = cnn.layerParamsFromFile(modelPath / ('l' + str(i))
    kaas.kv.put("layer" + str(i) + "_params", cnn.serialize(layers[i]))

# These are some representation of device-only code (maybe a library with thin
# wrappers, or some IR or just raw CUDA code). Our first pass will be a
# pre-compiled shared library with thin wrappers around the kernels.
#
# I have these being registered in the offline setup phase in this example.
# They can be dynamic if you want as well. You could imagine something like
# pytorch or onnxruntime compiling these and uploading them to kaas to start
# executing, much like they register the kernels with CUDA now.
kaas.register(cnn.layer0Forward)
kaas.register(cnn.layer1Forward)
kaas.register(cnn.layer2Forward)

#==============================================================================
# A hypothetical serving frontend would run something like the following.
#
# This is pretty similar to cloudburst, but with some key differences:
#   * The functions being put into the execution DAG are not arbitrary python
#     functions, they are accelerator kernels.
#   * All memories are declared up-front. Sizes are all explicit. Data sources
#     are explicit.
#   * The kernels may be quite fine-grained, but they are described in such a
#     way that they can be fused easily. Depending on how far we push the PL side
#     of things, our system could even fuse them into the same binary or kernels.
#   * There are no implicit code dependencies, no packages, no python versions,
#     nothing. The only dependencies are the code itself and the input/output
#     buffers.
#
# That being said, a chain of kernel invocations could be just a node in a
# normal cloudburst graph, and the two would be co-optimized (e.g. the kernel
# would preferentially run where the inputs are hot in the Anna cache or where
# the next function is going to run).
#==============================================================================

# Session is going to act like CUDA graphs, it's just an easy way to describe
# the execution graph. You aren't really calling functions in python, more just
# registering the dataflow. the cnn.layerXForward functions are actually
# accelerator kernels, not Python functions. There's probably a better way to
# do this, but it looks kind of like cloudburst for now.
#
# In practice, these would probably be generated by some ML framework and
# called by the user as if they were a normal function.
sess = kaas.session()

sess.call(cnn.layer0Forward,
    # Inputs are loaded into device memory from the KV store, a pointer to them
    # will be handed to the kernel. Here we pass all the imgs, but we'd
    # probably break them into separate keys per batch, or specify a subset of
    # a shared array abstraction
    inputs=["imgs", "layer0_params"],

    # Kernels cannot call cudaMalloc, they have to pre-declare any memories
    # they need. This buffer will be provided to the kernel and filled with
    # zeros. It will not be saved after.
    tmps=[kaas.buffer("act", layers[0].actSize, ephemeral=True)],

    # Like tmps, outputs are allocated by KaaS, but they will be saved to the
    # KV after the kernel finishes. They can be the same key as an input, in
    # which case no new buffer will be allocated, instead the kernel intends to
    # mutate the input buffer and save it somewhere else after.
    outputs=[kaas.buffer("layer0_output", layers[0].outSize)]
    )

sess.call(cnn.layer1Forward,
    inputs=["imgs", "layer1_params"],
    tmps=[kaas.buffer("act", layers[1].actSize, ephemeral=True)],
    outputs=[kaas.buffer("layer1_output", layers[1].outSize)]
    )

sess.call(cnn.layer2Forward,
    inputs=["imgs", "layer2_params"],
    tmps=[kaas.buffer("act", layers[2].actSize)],
    outputs=[kaas.buffer("layer2_output", layers[2].outSize)]
    )

# KaaS processes the graph and does device packing as needed, makes sure inputs
# are where they belong and dedups any buffers (none in this example but you
# could imagine multiple copies of this graph running or layers/inputs being
# shared between different graphs). In this example, everything would get run
# sequentially on the same device, with the inputs/outputs being passed by
# reference.
sess.run()

# This could also be a regular cloud function. One could imagine in a
# cloudburst like system, this would just be part of the dataflow graph like
# the kernels.
pred = cnn.postProcess(kaas.kv.get("layer2_output"))
